{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import cholesky \n",
    "\n",
    "with open('reviews_Movies_and_TV_5.json','r')as f:\n",
    "    rawdata=[JSONDecoder().decode(x) for x in f.readlines()] \n",
    "    \n",
    "for i in range(len(rawdata)):\n",
    "    rawdata[i]['text']=[word_tokenize(x) for x in sent_tokenize(rawdata[i]['reviewText'].lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,999999]}\n",
    "for i in rawdata:\n",
    "    for k in i['text']:\n",
    "        for j in k:\n",
    "            if j in word_dict:\n",
    "                word_dict[j][1]+=1\n",
    "            else:\n",
    "                word_dict[j]=[len(word_dict),1]\n",
    "                \n",
    "\n",
    "word_dict_freq={}\n",
    "for x in  word_dict:\n",
    "    if word_dict[x][1]>=10:\n",
    "        word_dict_freq[x]=[len(word_dict_freq),word_dict[x][1]]\n",
    "print(len(word_dict_freq),len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdict={}\n",
    "cnt=0\n",
    "with open('glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if len(line)==0:\n",
    "            break\n",
    "        line = line.split() \n",
    "        word=line[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            emb=[float(x) for x in line[1:]] \n",
    "            if word in word_dict_freq:\n",
    "                embdict[word]=emb\n",
    "                if cnt%100==0:\n",
    "                    print(cnt,linenb,word)\n",
    "                cnt+=1\n",
    "\n",
    "\n",
    "print(len(embdict),len(word_dict_freq))\n",
    "print(len(word_dict_freq))\n",
    "\n",
    "\n",
    "emb_mat=[0]*len(word_dict_freq)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "temp_emb=[]\n",
    "for i in embdict.keys():\n",
    "    emb_mat[word_dict_freq[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    temp_emb.append(emb_mat[word_dict_freq[i][0]])\n",
    "temp_emb=np.array(temp_emb,dtype='float32')\n",
    "\n",
    "mu=np.mean(temp_emb, axis=0)\n",
    "Sigma=np.cov(temp_emb.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "\n",
    "for i in range(len(emb_mat)):\n",
    "    if type(emb_mat[i])==int:\n",
    "        emb_mat[i]=np.reshape(norm, 300)\n",
    "emb_mat[0]=np.zeros(300,dtype='float32')\n",
    "emb_mat=np.array(emb_mat,dtype='float32')\n",
    "print(emb_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "uir_triples=[]\n",
    "for i in rawdata:\n",
    "    temp={}\n",
    "    doc=[] \n",
    "    for y in i['text']:\n",
    "        doc.append([word_dict_freq[x][0] for x in y if x in word_dict_freq])\n",
    "    temp['text']=doc\n",
    "    temp['item']=i['asin']\n",
    "    temp['user']=i['reviewerID']\n",
    "    uir_triples.append(temp)\n",
    "\n",
    "for i in range(len(uir_triples)):\n",
    "    uir_triples[i]['id']=i\n",
    "\n",
    "for i in range(len(rawdata)):\n",
    "    uir_triples[i]['label']=rawdata[i]['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 40\n",
    "MAX_SENTS = 15\n",
    "MAX_REVIEW_USER = 40\n",
    "MAX_REVIEW_ITEM = 50\n",
    "MAX_NEIGHBOR = 75\n",
    "\n",
    "\n",
    "import random\n",
    "for i in item_review_id:\n",
    "    random.shuffle(item_review_id[i])\n",
    "\n",
    "import random\n",
    "for i in user_review_id:\n",
    "    random.shuffle(user_review_id[i])\n",
    "\n",
    "\n",
    "indices = np.arange(len(uir_triples))\n",
    "np.random.shuffle(indices)\n",
    "train_uir=uir_triples[indices[:int(0.8*len(indices))]] \n",
    "val_uir=uir_triples[indices[int(0.8*len(indices)):int(0.9*len(indices))]] \n",
    "test_uir=uir_triples[indices[int(0.9*len(indices)):]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_review_id={}\n",
    "user_review_id={}\n",
    "\n",
    "for i in train_uir:\n",
    "    if i['item'] in item_review_id:\n",
    "        \n",
    "        item_review_id[i['item']].append(i['id'])\n",
    "    else:\n",
    "        item_review_id[i['item']]=[i['id']]\n",
    "       \n",
    "    if i['user'] in user_review_id:\n",
    "        \n",
    "        user_review_id[i['user']].append(i['id'])\n",
    "    else:\n",
    "        user_review_id[i['user']]=[i['id']]        \n",
    "\n",
    "all_user_texts=[] \n",
    "for i in user_review_id:\n",
    "    pad_docs=[] \n",
    "    for j in user_review_id[i][:MAX_REVIEW_USER]:\n",
    "        \n",
    "        sents=[x[:MAX_SENT_LENGTH] for x in uir_triples[j]['text'][:MAX_SENTS]]\n",
    "        pad_sents=[x+(MAX_SENT_LENGTH-len(x))*[0] for x in sents]\n",
    "        pad_docs.append(pad_sents+[[0]*MAX_SENT_LENGTH]*(MAX_SENTS-len(pad_sents)))\n",
    "    all_user_texts.append(pad_docs+[[[0]*MAX_SENT_LENGTH]*MAX_SENTS]*(MAX_REVIEW_USER-len(pad_docs)))\n",
    "\n",
    "\n",
    "all_item_texts=[] \n",
    "for i in item_review_id:\n",
    "    pad_docs=[] \n",
    "    for j in item_review_id[i][:MAX_REVIEW_ITEM]:\n",
    "        \n",
    "        sents=[x[:MAX_SENT_LENGTH] for x in uir_triples[j]['text'][:MAX_SENTS]]\n",
    "        pad_sents=[x+(MAX_SENT_LENGTH-len(x))*[0] for x in sents]\n",
    "        pad_docs.append(pad_sents+[[0]*MAX_SENT_LENGTH]*(MAX_SENTS-len(pad_sents)))\n",
    "    all_item_texts.append(pad_docs+[[[0]*MAX_SENT_LENGTH]*MAX_SENTS]*(MAX_REVIEW_ITEM-len(pad_docs)))\n",
    "\n",
    "\n",
    "\n",
    "all_user_texts=np.array(all_user_texts,dtype='int32')\n",
    "all_item_texts=np.array(all_item_texts,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_dict={}\n",
    "cnt=0\n",
    "for i in item_review_id:\n",
    "    item_id_dict[i]=cnt\n",
    "    cnt+=1\n",
    "\n",
    "user_id_dict={}\n",
    "cnt=0\n",
    "for i in user_review_id:\n",
    "    user_id_dict[i]=cnt\n",
    "    cnt+=1\n",
    "\n",
    "item_to_user_id={}\n",
    "\n",
    "for i in train_uir:     \n",
    "    if item_id_dict[i['item']] in item_to_user_id:\n",
    "        item_to_user_id[item_id_dict[i['item']]].append(user_id_dict[i['user']])\n",
    "    else:\n",
    "        item_to_user_id[item_id_dict[i['item']]]=[user_id_dict[i['user']]]\n",
    "        \n",
    "user_to_item_id={}\n",
    "\n",
    "for i in train_uir:     \n",
    "    if user_id_dict[i['user']] in user_to_item_id:\n",
    "        user_to_item_id[user_id_dict[i['user']]].append(item_id_dict[i['item']])\n",
    "    else:\n",
    "        user_to_item_id[user_id_dict[i['user']]]=[item_id_dict[i['item']]]\n",
    "\n",
    "\n",
    "user_to_item_to_user=[]\n",
    "user_to_item=[]\n",
    "for i in user_to_item_id:     \n",
    "    ids=[]\n",
    "    \n",
    "    ui_ids=user_to_item_id[i][:MAX_NEIGHBOR]\n",
    "    \n",
    "    for j in user_to_item_id[i]:\n",
    "        randids=random.sample(item_to_user_id[j],min(MAX_NEIGHBOR,len(item_to_user_id[j])))\n",
    "        \n",
    "        ids.append(randids+[len(user_to_item_id)+1]*(MAX_NEIGHBOR-len(randids)))\n",
    "    ids=ids[:MAX_NEIGHBOR]\n",
    "    user_to_item_to_user.append(ids+[[len(user_to_item_id)+1]*MAX_NEIGHBOR]*(MAX_NEIGHBOR-len(ids)))\n",
    "    \n",
    "    user_to_item.append(ui_ids+[len(item_to_user_id)+1]*(MAX_NEIGHBOR-len(ui_ids)))\n",
    "\n",
    "user_to_item_to_user=np.array(user_to_item_to_user,dtype='int32')\n",
    "\n",
    "user_to_item=np.array(user_to_item,dtype='int32')\n",
    "\n",
    "item_to_user_to_item=[]\n",
    "item_to_user=[]\n",
    "for i in item_to_user_id:     \n",
    "    ids=[]\n",
    "    \n",
    "    iu_ids=item_to_user_id[i][:MAX_NEIGHBOR]\n",
    "    \n",
    "    for j in item_to_user_id[i]:\n",
    "        randids=random.sample(user_to_item_id[j],min(MAX_NEIGHBOR,len(user_to_item_id[j])))\n",
    "        \n",
    "        ids.append(randids+[len(item_to_user_id)+1]*(MAX_NEIGHBOR-len(randids)))\n",
    "    ids=ids[:MAX_NEIGHBOR]\n",
    "    item_to_user_to_item.append(ids+[[len(item_to_user_id)+1]*MAX_NEIGHBOR]*(MAX_NEIGHBOR-len(ids)))\n",
    "    \n",
    "    item_to_user.append(iu_ids+[len(user_to_item_id)+1]*(MAX_NEIGHBOR-len(iu_ids)))\n",
    "\n",
    "\n",
    "item_to_user_to_item=np.array(item_to_user_to_item,dtype='int32')\n",
    "item_to_user=np.array(item_to_user,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_id=[]\n",
    "train_user_id=[]\n",
    "train_label=[]\n",
    "for i in train_uir:\n",
    "    train_item_id.append(item_id_dict[i['item']])\n",
    "    train_user_id.append(user_id_dict[i['user']])\n",
    "    train_label.append(i['label'])\n",
    "    \n",
    "    \n",
    "val_item_id=[]\n",
    "val_user_id=[]\n",
    "val_label=[]\n",
    "for i in val_uir:\n",
    "    val_item_id.append(item_id_dict[i['item']])\n",
    "    val_user_id.append(user_id_dict[i['user']])\n",
    "    val_label.append(i['label'])\n",
    "    \n",
    "test_item_id=[]\n",
    "test_user_id=[]\n",
    "test_label=[]\n",
    "for i in test_uir:\n",
    "    test_item_id.append(item_id_dict[i['item']])\n",
    "    test_user_id.append(user_id_dict[i['user']])\n",
    "    test_label.append(i['label'])\n",
    "\n",
    "label=np.array(label,dtype='float32')\n",
    "item_id =np.array(item_id,dtype='int32')\n",
    "user_id =np.array(user_id,dtype='int32')\n",
    "\n",
    "val_label=np.array(val_label,dtype='float32')\n",
    "val_item_id =np.array(val_item_id,dtype='int32')\n",
    "val_user_id =np.array(val_user_id,dtype='int32')\n",
    "\n",
    "test_label=np.array(test_label,dtype='float32')\n",
    "test_item_id =np.array(test_item_id,dtype='int32')\n",
    "test_user_id =np.array(test_user_id,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_random(item,user,user_to_item_to_user,ui,item_to_user_to_item,iu,item_id,user_id, y, batch_size):\n",
    "    idx = np.arange(len(y))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches: \n",
    "            yield ([item[item_id[i]],user[user_id[i]],user_to_item_to_user[user_id[i]],\n",
    "                    user_to_item[user_id[i]],item_to_user_to_itemx,item_to_user[item_id[i]],\n",
    "                    np.expand_dims(item_id[i],axis=1),np.expand_dims(user_id[i],axis=1)], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict_freq), 300, weights=[lister],trainable=True)\n",
    "\n",
    "embedded_sequences = Dropout(0.2)(embedding_layer(sentence_input))\n",
    "\n",
    "word_cnn_fea = Dropout(0.2)(Convolution1D(nb_filter=100, filter_length=3,  padding='same', activation='relu', strides=1)(embedded_sequences))\n",
    "\n",
    "word_att = Dense(100,activation='tanh')(word_cnn_fea)\n",
    "word_att = Flatten()(Dense(1)(word_att))\n",
    "word_att = Activation('softmax')(word_att)\n",
    "sent_emb=Dot((1, 1))([word_cnn_fea, word_att])\n",
    "\n",
    "sent_encoder = Model([sentence_input], sent_emb)\n",
    "\n",
    "review_input = keras.Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "review_encoder = TimeDistributed(sent_encoder)(review_input)\n",
    "\n",
    "sent_cnn_fea = Dropout(0.2)(Convolution1D(nb_filter=100, filter_length=3, padding='same', activation='relu', strides=1)(review_encoder))\n",
    "\n",
    "sent_att = Dense(100,activation='tanh')(sent_cnn_fea)\n",
    "sent_att = Flatten()(Dense(1)(sent_att))\n",
    "word_att = Activation('softmax')(sent_att)\n",
    "doc_emb=keras.layers.Dot((1, 1))([sent_cnn_fea, word_att])\n",
    "\n",
    "doc_encoder = Model([review_input], doc_emb)\n",
    "\n",
    "\n",
    "reviews_input_item = keras.Input((MAX_REVIEW_ITEM,MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32') \n",
    "reviews_input_user = keras.Input((MAX_REVIEW_USER,MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "reviews_emb_item = TimeDistributed(doc_encoder)(reviews_input_item)\n",
    "reviews_emb_user = TimeDistributed(doc_encoder)(reviews_input_user)\n",
    "\n",
    "\n",
    "doc_att = Dense(100,activation='tanh')(reviews_emb_item)\n",
    "doc_att = Flatten()(Dense(1)(doc_att))\n",
    "doc_att = Activation('softmax')(doc_att)\n",
    "item_emb=keras.layers.Dot((1, 1))([reviews_emb_item, doc_att])\n",
    "\n",
    "doc_att_u = Dense(100,activation='tanh')(reviews_emb_user)\n",
    "doc_att_u = Flatten()(Dense(1)(doc_att_u))\n",
    "doc_att_u = Activation('softmax')(doc_att_u)\n",
    "user_emb=keras.layers.Dot((1, 1))([reviews_emb_user, doc_att_u])\n",
    "\n",
    "user_id = Input(shape=(1,), dtype='int32')\n",
    "item_id = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "\n",
    "user_embedding= Embedding(len(user_review_id),100,trainable=True)\n",
    "item_embedding = Embedding(len(item_review_id), 100,trainable=True)\n",
    "\n",
    "user_item_ids = keras.Input((MAX_NEIGHBOR,), dtype='int32')\n",
    "item_user_ids = keras.Input((MAX_NEIGHBOR,), dtype='int32')\n",
    "\n",
    "user_item_user_ids = keras.Input((MAX_NEIGHBOR,MAX_NEIGHBOR), dtype='int32')\n",
    "item_user_item_ids = keras.Input((MAX_NEIGHBOR,MAX_NEIGHBOR), dtype='int32')\n",
    "\n",
    "user_item_embedding= user_embedding(user_item_ids)\n",
    "item_user_embedding= item_embedding(item_user_ids)\n",
    "\n",
    "ui_att = Dense(100,activation='tanh')(user_item_embedding)\n",
    "ui_att = Flatten()(Dense(1)(ui_att))\n",
    "ui_att = Activation('softmax')(ui_att)\n",
    "ui_emb=keras.layers.Dot((1, 1))([user_item_embedding, ui_att])\n",
    "\n",
    "iu_att = Dense(100,activation='tanh')(item_user_embedding)\n",
    "iu_att = Flatten()(Dense(1)(iu_att))\n",
    "iu_att_weight = Activation('softmax')(iu_att)\n",
    "iu_emb=keras.layers.Dot((1, 1))([item_user_embedding, iu_att_weight])\n",
    "\n",
    "userencoder = Model([user_item_ids], ui_emb)\n",
    "itemencoder = Model([item_user_ids], iu_emb)\n",
    "\n",
    "user_encoder = TimeDistributed(userencoder)(user_item_user_ids)\n",
    "item_encoder = TimeDistributed(itemencoder)(item_user_item_ids)\n",
    "\n",
    "ufactor=concatenate([user_item_embedding,user_encoder])\n",
    "ifactor=concatenate([item_user_embedding,item_encoder])\n",
    "\n",
    "un_att = Dense(100,activation='tanh')(ufactor)\n",
    "un_att = Flatten()(Dense(1)(un_att))\n",
    "un_att = Activation('softmax')(un_att)\n",
    "user_emb_g=keras.layers.Dot((1, 1))([ufactor, un_att])\n",
    "\n",
    "in_att = Dense(100,activation='tanh')(ifactor)\n",
    "in_att = Flatten()(Dense(1)(in_att))\n",
    "in_att = Activation('softmax')(in_att)\n",
    "item_emb_g=keras.layers.Dot((1, 1))([ifactor, in_att])\n",
    "\n",
    "\n",
    "user_embedding= Flatten()(user_embedding(user_id))\n",
    "item_embedding= Flatten()(item_embedding(item_id))\n",
    "factor_u=concatenate([user_emb,user_embedding,user_emb_g])\n",
    "factor_i=concatenate([item_emb,item_embedding,item_emb_g])\n",
    "\n",
    "preds=Dense(1,activation='relu')(multiply([factor_u,factor_i]))\n",
    "\n",
    "model = Model([reviews_input_item,reviews_input_user,user_item_user_ids,user_item_ids,item_user_item_ids,item_user_ids,item_id,user_id], preds)\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n",
    "\n",
    "\n",
    "for ep in range(1):\n",
    "    traingen=generate_batch_data_random(all_item_texts,all_user_texts,user_to_item_to_user,ui,item_to_user_to_item,iu,train_item_id,train_user_id,train_label,24)\n",
    "    valgen=generate_batch_data_random(all_item_texts,all_user_texts,user_to_item_to_user,ui,item_to_user_to_item,iu,test_item_id,test_user_id,test_label,24)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(train_item_id)//24)\n",
    "    cr = model.evaluate_generator(valgen, steps=len(test_item_id)//24)\n",
    "    print(np.sqrt(cr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
